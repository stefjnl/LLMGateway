# US-003B: Application Layer Refinements

---

## User Story

**As a** developer  
**I want to** refine the Application layer to fix architectural inconsistencies  
**So that** cost calculation has a single source of truth and edge cases are properly handled

---

## Problem Statement

Current US-003 implementation has three issues:

1. **Cost calculation split**: CostTrackingPlugin calculates cost but ChatResponse.EstimatedCostUsd is set to 0m with a TODO comment. Unclear which is source of truth.

2. **Fallback edge case**: When ModelSelectionPlugin chooses Kimi-K2 (large context), fallback chain immediately throws AllProvidersFailedException because Kimi-K2 is last in chain. Only 1 retry attempt instead of 3.

3. **Metadata assumption**: Token extraction assumes SK metadata exists, but this won't be populated until US-004 implements OpenRouterChatCompletionService. Creates brittle dependency.

---

## Acceptance Criteria

### ✅ Criterion 1: Cost Flow Clarified
- [ ] CostTrackingPlugin calculates cost and returns it (not void)
- [ ] KernelOrchestrator receives calculated cost from CostTrackingPlugin
- [ ] ChatResponse.EstimatedCostUsd populated with actual cost (not 0m)
- [ ] Single source of truth for cost calculation

### ✅ Criterion 2: Fallback Chain Handles All Starting Points
- [ ] Fallback chain works regardless of initial model selection
- [ ] If Kimi-K2 selected initially, fallback tries DeepSeek → glm-4.6
- [ ] If DeepSeek selected initially, fallback tries Kimi-K2 → glm-4.6
- [ ] All users get 3 retry attempts maximum
- [ ] ProviderFallbackPlugin cycles through all models before throwing

### ✅ Criterion 3: Token Extraction Made Resilient
- [ ] Token extraction handles missing metadata gracefully
- [ ] Falls back to estimation if metadata unavailable
- [ ] Logs warning when estimation used
- [ ] No runtime exceptions if metadata missing

### ✅ Criterion 4: Tests Updated
- [ ] CostTrackingPlugin tests verify cost return value
- [ ] Fallback tests cover all starting models (glm-4.6, DeepSeek, Kimi-K2)
- [ ] Orchestrator tests verify ChatResponse.EstimatedCostUsd is populated
- [ ] Token extraction tests cover missing metadata scenario

---

## Implementation Steps

### Step 1: Fix Cost Calculation Flow (10 minutes)

**Modify**: `src/LLMGateway.Application/Plugins/CostTrackingPlugin.cs`

**Change method signature to return cost:**

```csharp
[KernelFunction("track_cost")]
[Description("Tracks the cost and details of a completed request")]
public async Task<decimal> TrackCostAsync(
    [Description("Model used for the request")] string modelName,
    [Description("Number of input tokens")] int inputTokens,
    [Description("Number of output tokens")] int outputTokens,
    [Description("Provider name (e.g., OpenRouter)")] string providerName,
    [Description("Response time in milliseconds")] long responseTimeMs,
    [Description("Whether fallback was used")] bool wasFallback,
    CancellationToken cancellationToken = default)
{
    try
    {
        var model = ModelName.From(modelName);
        var inputTokenCount = TokenCount.From(inputTokens);
        var outputTokenCount = TokenCount.From(outputTokens);
        
        // Fetch pricing information
        var pricing = await _pricingRepository.GetByModelAsync(model, cancellationToken);
        
        CostAmount cost;
        if (pricing == null)
        {
            _logger.LogWarning(
                "No pricing information found for model {Model}. Using zero cost.",
                modelName);
            cost = CostAmount.Zero;
        }
        else
        {
            cost = pricing.CalculateCost(inputTokenCount, outputTokenCount);
        }
        
        // Create and persist request log
        var log = RequestLog.Create(
            model,
            inputTokenCount,
            outputTokenCount,
            cost,
            providerName,
            TimeSpan.FromMilliseconds(responseTimeMs),
            wasFallback);
        
        await _logRepository.SaveAsync(log, cancellationToken);
        
        _logger.LogInformation(
            "Tracked request: Model={Model}, Tokens={Tokens}, Cost={Cost:F6}, Fallback={Fallback}",
            modelName,
            inputTokens + outputTokens,
            cost.ValueUsd,
            wasFallback);
        
        // Return calculated cost
        return cost.ValueUsd;
    }
    catch (Exception ex)
    {
        _logger.LogError(
            ex,
            "Failed to track cost for model {Model}",
            modelName);
        // Don't throw - return zero cost on tracking failure
        return 0m;
    }
}
```

**Update orchestrator**: `src/LLMGateway.Application/Orchestration/KernelOrchestrator.cs`

**Change Step 3 to capture and use returned cost:**

```csharp
// Step 3: Extract metadata and track cost
var inputTokens = ExtractInputTokens(result);
var outputTokens = ExtractOutputTokens(result);

var estimatedCost = await _costTracking.TrackCostAsync(
    finalModel,
    inputTokens,
    outputTokens,
    "OpenRouter",
    stopwatch.ElapsedMilliseconds,
    wasFallback: attempts > 1,
    cancellationToken);

// Step 4: Map to response
return new ChatResponse
{
    Content = result.Content ?? string.Empty,
    Model = finalModel,
    TokensUsed = inputTokens + outputTokens,
    EstimatedCostUsd = estimatedCost, // Now populated with actual cost
    ResponseTime = stopwatch.Elapsed
};
```

---

### Step 2: Fix Fallback Chain Edge Case (15 minutes)

**Problem**: Current fallback chain is linear. If you start at end, you're stuck.

**Solution**: Make chain truly circular with tracking to prevent infinite loops.

**Modify**: `src/LLMGateway.Application/Plugins/ProviderFallbackPlugin.cs`

```csharp
using LLMGateway.Domain.Constants;
using LLMGateway.Domain.Exceptions;
using Microsoft.Extensions.Logging;
using Microsoft.SemanticKernel;

namespace LLMGateway.Application.Plugins;

public class ProviderFallbackPlugin
{
    private readonly ILogger<ProviderFallbackPlugin> _logger;
    
    public ProviderFallbackPlugin(ILogger<ProviderFallbackPlugin> logger)
    {
        _logger = logger;
    }
    
    [KernelFunction("get_fallback_model")]
    [Description("Gets the next model in the fallback chain after a failure")]
    public Task<string> GetFallbackModelAsync(
        [Description("The model that failed")] string failedModel,
        [Description("List of already attempted models")] IReadOnlyList<string> attemptedModels)
    {
        var fallbackChain = ModelDefaults.FallbackChain;
        
        // Check if we've exhausted all options
        if (attemptedModels.Count >= fallbackChain.Length)
        {
            _logger.LogError(
                "All providers failed. Attempted models: {Models}",
                string.Join(", ", attemptedModels));
            
            throw new AllProvidersFailedException(attemptedModels.ToArray());
        }
        
        // Find next model that hasn't been attempted
        string? nextModel = null;
        var currentIndex = Array.IndexOf(fallbackChain, failedModel);
        
        // Start from next index (circular)
        for (int i = 1; i <= fallbackChain.Length; i++)
        {
            var checkIndex = (currentIndex + i) % fallbackChain.Length;
            var candidateModel = fallbackChain[checkIndex];
            
            if (!attemptedModels.Contains(candidateModel))
            {
                nextModel = candidateModel;
                break;
            }
        }
        
        if (nextModel == null)
        {
            _logger.LogError(
                "No untried models remaining. Attempted: {Models}",
                string.Join(", ", attemptedModels));
            
            throw new AllProvidersFailedException(attemptedModels.ToArray());
        }
        
        _logger.LogWarning(
            "Falling back from {FailedModel} to {NextModel}",
            failedModel,
            nextModel);
        
        return Task.FromResult(nextModel);
    }
}
```

**Update orchestrator retry loop**: `src/LLMGateway.Application/Orchestration/KernelOrchestrator.cs`

```csharp
private async Task<(ChatMessageContent result, string model, int attempts)> ExecuteWithFallbackAsync(
    Kernel kernel,
    SendChatCompletionCommand command,
    string initialModel,
    CancellationToken cancellationToken)
{
    var currentModel = initialModel;
    var attempts = 0;
    var maxAttempts = 3;
    var attemptedModels = new List<string> { initialModel };
    
    while (attempts < maxAttempts)
    {
        attempts++;
        
        try
        {
            var chatHistory = BuildChatHistory(command.Messages);
            var completionService = kernel.GetRequiredService<IChatCompletionService>();
            
            var executionSettings = new PromptExecutionSettings
            {
                ModelId = currentModel,
                ExtensionData = new Dictionary<string, object>
                {
                    ["temperature"] = (double)(command.Temperature ?? 0.7m),
                    ["max_tokens"] = command.MaxTokens ?? 2000
                }
            };
            
            var results = await completionService.GetChatMessageContentsAsync(
                chatHistory,
                executionSettings,
                kernel,
                cancellationToken);
            
            return (results.First(), currentModel, attempts);
        }
        catch (Exception ex) when (IsTransientError(ex) && attempts < maxAttempts)
        {
            _logger.LogWarning(
                ex,
                "Transient error on attempt {Attempt}/{MaxAttempts} for model {Model}",
                attempts,
                maxAttempts,
                currentModel);
            
            // Get fallback model with attempt history
            currentModel = await _providerFallback.GetFallbackModelAsync(
                currentModel, 
                attemptedModels);
            
            attemptedModels.Add(currentModel);
        }
    }
    
    throw new AllProvidersFailedException(attemptedModels.ToArray());
}
```

---

### Step 3: Make Token Extraction Resilient (10 minutes)

**Modify**: `src/LLMGateway.Application/Orchestration/KernelOrchestrator.cs`

**Update token extraction methods with better fallback:**

```csharp
private int ExtractInputTokens(ChatMessageContent result)
{
    // Try to extract from metadata if available
    if (result.Metadata?.TryGetValue("input_tokens", out var inputTokens) == true)
    {
        try
        {
            return Convert.ToInt32(inputTokens);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(
                ex,
                "Failed to parse input_tokens from metadata: {Value}",
                inputTokens);
        }
    }
    
    // Fallback: estimate from prompt (will be refined in US-004)
    _logger.LogDebug("Input tokens not in metadata, estimation will be used");
    return 0; // Will be estimated in Infrastructure layer
}

private int ExtractOutputTokens(ChatMessageContent result)
{
    // Try to extract from metadata if available
    if (result.Metadata?.TryGetValue("output_tokens", out var outputTokens) == true)
    {
        try
        {
            return Convert.ToInt32(outputTokens);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(
                ex,
                "Failed to parse output_tokens from metadata: {Value}",
                outputTokens);
        }
    }
    
    // Fallback: estimate from content length
    var contentLength = result.Content?.Length ?? 0;
    var estimated = contentLength / 4;
    
    if (estimated > 0)
    {
        _logger.LogDebug(
            "Output tokens not in metadata, using estimation: {Estimated}",
            estimated);
    }
    
    return estimated;
}
```

---

### Step 4: Update Tests (20 minutes)

**Update**: `tests/LLMGateway.Application.Tests/Plugins/CostTrackingPluginTests.cs`

**Add test for cost return value:**

```csharp
[Fact]
public async Task TrackCost_ReturnsCostValue()
{
    // Arrange
    var modelName = ModelDefaults.DefaultModel;
    var inputTokens = 1_000_000;
    var outputTokens = 1_000_000;
    
    // Act
    var returnedCost = await _plugin.TrackCostAsync(
        modelName,
        inputTokens,
        outputTokens,
        "OpenRouter",
        1000,
        wasFallback: false);
    
    // Assert
    returnedCost.Should().Be(0.0003m);
}

[Fact]
public async Task TrackCost_TrackingFailure_ReturnsZero()
{
    // Arrange
    var mockLogger = new Mock<ILogger<CostTrackingPlugin>>();
    var mockLogRepo = new Mock<IRequestLogRepository>();
    mockLogRepo
        .Setup(x => x.SaveAsync(It.IsAny<RequestLog>(), It.IsAny<CancellationToken>()))
        .ThrowsAsync(new Exception("Database error"));
    
    var plugin = new CostTrackingPlugin(
        mockLogRepo.Object,
        _pricingRepository,
        mockLogger.Object);
    
    // Act
    var returnedCost = await plugin.TrackCostAsync(
        ModelDefaults.DefaultModel,
        100,
        200,
        "OpenRouter",
        1000,
        wasFallback: false);
    
    // Assert
    returnedCost.Should().Be(0m);
}
```

**Update**: `tests/LLMGateway.Application.Tests/Plugins/ProviderFallbackPluginTests.cs`

```csharp
[Fact]
public async Task GetFallbackModel_StartingFromKimiK2_TriesDeepSeek()
{
    // Arrange
    var attemptedModels = new List<string> { ModelDefaults.LargeContextModel };
    
    // Act
    var result = await _plugin.GetFallbackModelAsync(
        ModelDefaults.LargeContextModel,
        attemptedModels);
    
    // Assert
    result.Should().Be(ModelDefaults.BalancedModel);
}

[Fact]
public async Task GetFallbackModel_AllModelsAttempted_ThrowsException()
{
    // Arrange
    var attemptedModels = new List<string>
    {
        ModelDefaults.DefaultModel,
        ModelDefaults.BalancedModel,
        ModelDefaults.LargeContextModel
    };
    
    // Act
    var act = () => _plugin.GetFallbackModelAsync(
        ModelDefaults.LargeContextModel,
        attemptedModels);
    
    // Assert
    await act.Should().ThrowAsync<AllProvidersFailedException>()
        .Where(ex => ex.AttemptedProviders.Count == 3);
}

[Fact]
public async Task GetFallbackModel_CircularFallback_SkipsAttemptedModels()
{
    // Arrange - already tried default and balanced
    var attemptedModels = new List<string>
    {
        ModelDefaults.DefaultModel,
        ModelDefaults.BalancedModel
    };
    
    // Act - failed on balanced, should circle back to Kimi-K2
    var result = await _plugin.GetFallbackModelAsync(
        ModelDefaults.BalancedModel,
        attemptedModels);
    
    // Assert
    result.Should().Be(ModelDefaults.LargeContextModel);
}
```

**Update**: `tests/LLMGateway.Application.Tests/Orchestration/KernelOrchestratorTests.cs`

```csharp
[Fact]
public async Task SendChatCompletion_PopulatesEstimatedCost()
{
    // Arrange
    var command = new SendChatCompletionCommand(
        Messages: new[]
        {
            new Message { Role = "user", Content = "Hello" }
        });
    
    var mockResult = new ChatMessageContent(
        AuthorRole.Assistant,
        "Response")
    {
        Metadata = new Dictionary<string, object?>
        {
            ["input_tokens"] = 5,
            ["output_tokens"] = 10
        }
    };
    
    _mockCompletionService
        .Setup(x => x.GetChatMessageContentsAsync(
            It.IsAny<ChatHistory>(),
            It.IsAny<PromptExecutionSettings>(),
            It.IsAny<Kernel>(),
            It.IsAny<CancellationToken>()))
        .ReturnsAsync(new List<ChatMessageContent> { mockResult });
    
    // Act
    var response = await _orchestrator.SendChatCompletionAsync(command);
    
    // Assert
    response.EstimatedCostUsd.Should().BeGreaterThan(0);
}

[Fact]
public async Task SendChatCompletion_MissingMetadata_StillCompletes()
{
    // Arrange
    var command = new SendChatCompletionCommand(
        Messages: new[]
        {
            new Message { Role = "user", Content = "Hello" }
        });
    
    var mockResult = new ChatMessageContent(
        AuthorRole.Assistant,
        "Response with no metadata");
    // No metadata set
    
    _mockCompletionService
        .Setup(x => x.GetChatMessageContentsAsync(
            It.IsAny<ChatHistory>(),
            It.IsAny<PromptExecutionSettings>(),
            It.IsAny<Kernel>(),
            It.IsAny<CancellationToken>()))
        .ReturnsAsync(new List<ChatMessageContent> { mockResult });
    
    // Act
    var act = () => _orchestrator.SendChatCompletionAsync(command);
    
    // Assert
    await act.Should().NotThrowAsync();
}
```

---

### Step 5: Verify and Commit (10 minutes)

```bash
# Rebuild solution
dotnet build

# Run all tests
dotnet test

# Verify all tests pass
dotnet test tests/LLMGateway.Application.Tests --logger "console;verbosity=detailed"

# Commit changes
git add src/LLMGateway.Application/
git add tests/LLMGateway.Application.Tests/

git commit -m "refactor: Fix Application layer architectural issues (US-003B)

- CostTrackingPlugin now returns calculated cost (single source of truth)
- ChatResponse.EstimatedCostUsd populated with actual cost
- ProviderFallbackPlugin handles circular fallback with attempt tracking
- All starting models (glm-4.6, DeepSeek, Kimi-K2) get 3 retry attempts
- Token extraction resilient to missing metadata
- Added tests for cost return value, circular fallback, missing metadata
- Fixed edge case where Kimi-K2 selection had limited fallback options"

git push
```

---

## Definition of Done

- [x] CostTrackingPlugin returns decimal cost value
- [x] KernelOrchestrator uses returned cost for ChatResponse
- [x] ProviderFallbackPlugin accepts attemptedModels list
- [x] Fallback chain is truly circular with cycle detection
- [x] All starting models get 3 retry attempts
- [x] Token extraction handles missing metadata gracefully
- [x] Tests verify cost return value
- [x] Tests cover all fallback starting points
- [x] Tests verify ChatResponse.EstimatedCostUsd is populated
- [x] Tests cover missing metadata scenario
- [x] All tests pass
- [x] Code committed to GitHub

---

## Impact Analysis

**Breaking Changes:**
- `ProviderFallbackPlugin.GetFallbackModelAsync()` signature changed (added attemptedModels parameter)
- `CostTrackingPlugin.TrackCostAsync()` return type changed from Task to Task<decimal>

**Behavioral Changes:**
- Users starting with Kimi-K2 now get 3 attempts instead of 1
- ChatResponse.EstimatedCostUsd now contains actual cost instead of 0
- Missing token metadata no longer causes crashes

**Test Impact:**
- 6 new tests added
- 2 existing tests need signature updates

---

## Key Improvements

**Single Responsibility:**
- CostTrackingPlugin: Calculate cost AND return it for response
- No duplicate calculation logic

**Robustness:**
- Circular fallback ensures fair retry attempts regardless of starting model
- Missing metadata doesn't crash the system

**Clarity:**
- Cost flow is explicit: CostTracking calculates → Orchestrator uses → Response includes
- Fallback logic transparent with attempt tracking

This refinement strengthens the Application layer before Infrastructure integration in US-004.